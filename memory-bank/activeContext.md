# Active Context: Past Paper Concept Analyzer

## Current Focus

*   **Phase:** Initial Implementation / Prototyping.
*   **Activity:** Implementing batch download feature, refining CLI, and preparing for LLM integration.

## Recent Changes

*   **Commit:** `7b0ed35` (feat: Add batch download via spec file and update CLI)
*   **Batch Downloading:** Implemented `batch_parser.py` to parse specification files. Updated `cli.py` to include a `--batch-file` option for the `download` command, allowing users to download multiple paper solutions PDFs.
    *   The batch file supports year/paper ranges and optional course module hints.
    *   The download logic in `cli.py` iterates through parsed specifications and calls `downloader.download_pdf` (currently with a placeholder for `question_number` as "all", which needs reconciliation with `downloader.py`'s actual capabilities for fetching per-question solution PDFs).
*   **Selected Graph Backend:** Chose **NetworkX** with GraphML file persistence for the initial prototype due to simplicity.
*   **Project Structure:** Established (`src/past_paper_analyzer`, `main.py`, `data/`, `downloads/`).
*   **Core Modules Implemented:**
    *   `config.py`: Loads settings from `.env`, checks config, creates directories.
    *   `downloader.py`: (Summary provided) Function to download PDFs using `requests` and CL cookie.
    *   `llm_extractor.py`: Placeholder function for concept extraction.
    *   `graph_store.py`: (Summary provided) Functions for NetworkX graph operations.
    *   `cli.py`: `argparse` CLI with `download`, `process`, `visualize` commands.
    *   `main.py`: Entry point script.
    *   `batch_parser.py`: Parses batch download specification files.
*   **Environment:** `shell.nix` configured with necessary packages.

## Next Steps

1.  **Reconcile `downloader.py` with Batch Download Needs:**
    *   Update `downloader.download_pdf` or add a new function to correctly fetch individual *question-specific solution PDFs* based on `year`, `paper_code`, and `question_number` (not a placeholder like "all"). The batch parser provides `year` and `paper_code`; the `question_number` part of the specifier (e.g., `qXX`) needs to be integrated into the batch parsing or download loop if not already implicitly handled by the `paper_code` structure used by the CL website for *solutions*.
    *   **Clarification:** It's now understood that each solutions PDF from CL corresponds to a *single question* (e.g., `YYYY-pXX-qYY-solutions.pdf`). The batch download should generate these specific identifiers to download the correct single-question PDFs. The `question_placeholder = "all"` in `cli.py` for batch downloads is incorrect if the goal is to download individual question PDFs; it should derive the specific question number.
    *   The `batch_parser.py` currently generates `paper_code` like `p06`. If the download URL or local filename needs `qXX`, this needs to be added to the items generated by `batch_parser.py` or constructed in `cli.py`'s download loop.
2.  **Implement LLM Interaction:** Replace the placeholder in `llm_extractor.py` with actual calls to a Vision LLM API (e.g., OpenAI GPT-4o). This involves:
    *   Choosing the specific API call/library usage.
    *   Developing the initial prompt strategy for a single-question PDF.
    *   Handling PDF data input to the Vision LLM.
    *   Parsing the LLM's JSON response robustly.
    *   Adding error handling.
3.  **Refine `process` Command & Metadata:**
    *   Ensure `tripos_part` is reliably captured (e.g., require as argument, infer from paper code patterns if possible, or add to batch file spec).
    *   Confirm that the `process` command correctly uses metadata for a single-question PDF. The previous concern about multi-question PDFs is resolved; each PDF is for one question.
    *   Integrate `course_hint` from batch downloads into the `process` command and potentially pass it to `llm_extractor.py`.
4.  **Improve Concept Canonicalization:** Move beyond simple lowercasing in `graph_store.py`.
5.  **Testing:** Add tests for batch file parsing, download logic (mocked), core graph operations, and CLI argument parsing.

## Active Decisions & Considerations

*   **Batch File Specifier for Questions:** The current `batch_parser.py` uses `y<YYYY>p<XX>`. If individual question PDFs (e.g., `y<YYYY>p<XX>q<ZZ>`) are the target, the specifier and parser need to include question numbers or ranges (e.g., `y2022p06q[01-03]`). This seems to be the case based on the "one PDF per question" rule.
    *   **Decision:** The batch download feature should aim to download these specific single-question solution PDFs. The `batch_parser.py` and its specifiers (`SPECIFIER_PATTERN`) need to be updated to include question numbers/ranges.
*   **LLM Prompting for Single-Question PDFs:** This is simplified as the LLM context is one question at a time.
*   **PDF to LLM Input:** Decide on the best method (direct bytes or images).
*   **Error Handling:** Continue improving for batch operations and LLM calls.

## Important Patterns & Preferences

*   Maintain separation of concerns between modules.
*   Use type hinting in Python code.
*   Provide informative error messages and exit codes in the CLI.
*   Keep Memory Bank updated after significant changes.

## Learnings & Insights

*   Batch downloading significantly enhances usability.
*   The "one PDF per question" clarification simplifies the `process` command's logic and LLM prompting strategy.
*   The `batch_parser.py` needs to be adapted to handle question-level granularity if that's how solution PDFs are structured and named on the CL website.
