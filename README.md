# Past Paper Concept Analyzer

A system for analyzing University of Cambridge Computer Science Tripos past papers (solutions PDFs) to extract, store, and analyze key concepts and themes.

## Goal

The primary goal is to build a structured database of concepts frequently appearing in CS Tripos papers (Parts IA, IB, and II). This aims to help current Cambridge Computer Science undergraduate students identify recurring topics and optimize their revision strategy.

The system will process official solutions PDFs (which typically include the questions) downloaded from the Computer Laboratory website (`cl.cam.ac.uk`). It will use a Vision Language Model (LLM), such as GPT-4o, to identify and extract key concepts from each question's solution.

## Features

*   **Automated Downloading:** Capability to download specific past paper solutions PDFs from the CL website (requires user authentication cookie).
*   **Concept Extraction:** Utilizes Vision LLMs to analyze PDFs and extract relevant computer science concepts.
*   **Structured Storage:** Stores extracted concepts and associated metadata in a queryable database.
*   **Subject Organization:** Keeps data organized by paper, question, year, and potentially the corresponding course module.

## How it Works

1.  **Download:** The tool fetches specified solutions PDFs from the CL website, using a user-provided authentication cookie for access.
2.  **Process:** Each PDF is sent to a Vision LLM.
3.  **Extract:** The LLM is prompted to identify and list the key concepts covered in each question/solution.
4.  **Store:** The extracted concepts, along with metadata, are stored in a database.

## Database Schema

The extracted data will be stored in a structured format. Key fields for each concept instance will include:

*   `concept_name`: The name of the concept (e.g., "Binary Search Tree", "TCP Handshake").
*   `paper_code`: Identifier for the specific exam paper (e.g., "2022-p06").
*   `question_number`: The question number within the paper (e.g., "q01").
*   `year`: The year of the exam paper.
*   `tripos_part`: The relevant part of the Tripos (IA, IB, II).
*   `course_module`: (Optional/Best Effort) The associated course module (e.g., "Algorithms").
*   `concept_definition`: (Optional) A brief definition or context generated by the LLM.

**Database Technology:**

*   **Initial Idea:** SQLite was considered for its simplicity and portability, allowing easy querying with tools like `sqlitebrowser`.
*   **Alternative:** A Graph Database (e.g., using Neo4j or libraries like NetworkX) is also being considered. This could offer powerful visualization capabilities, showing relationships between concepts, papers, and courses.

## Difficult Issues / Challenges

These are key challenges anticipated during development:

*   **Concept Canonicalization:** Ensuring consistent naming for the same concept across different papers and questions is crucial. For example, the LLM might extract "Bellman-Ford" from one paper and "Bellman-Ford-Moore algorithm" from another. Potential strategies include:
    *   Sophisticated LLM prompting to enforce a canonical naming scheme.
    *   Post-processing steps to normalize extracted concept names (e.g., using string similarity, synonym lists, or embedding comparisons).
    *   Manual review and mapping of extracted terms.
*   **Authentication for Downloads:** Accessing solutions PDFs requires authentication. The tool needs a secure and user-friendly way for users to provide their CL authentication cookie. The planned approach is via a configuration file (e.g., a `.env` file).
*   **Accuracy of LLM Extraction:** The quality and relevance of extracted concepts depend heavily on the LLM's capabilities and the prompting strategy used.
*   **Mapping Concepts to Courses:** Reliably associating extracted concepts with the specific course module they belong to might require additional logic or data sources.

## Project Status & Usage

*   **Status:** Currently in the **planning and design phase**.
*   **Language:** To be implemented in **Python**, leveraging its strong ecosystem for AI/LLM interactions and data processing.
*   **Interface:** Planned as a **Command-Line Interface (CLI)** tool.

## Setup & Authentication

*(Details to be added once implementation begins)*

The tool will require Python and relevant libraries. Users will need to provide their CL authentication cookie via a configuration file (e.g., `.env`) to enable the downloading of solutions papers. Example:

